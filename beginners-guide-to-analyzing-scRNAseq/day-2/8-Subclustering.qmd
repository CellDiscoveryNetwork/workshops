---
title: "8 - Subclustering"
author: "CDN team"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    toc_float: true
    toc-location: left
    toc-depth: 4
    html-math-method: katex
    embed-resources: true
    self-contained-math: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "100%", fig.align='center', 
                      message = FALSE, warning = FALSE, cache = FALSE)
options(width = 1200)
options(repos = c(CRAN = "https://cloud.r-project.org"))
```


## Introduction

In this vignette we will examine methods for increasing resolution on cell subtypes and cell states. We will compare two methods: increasing resolution and other parameters to find more clusters, and subclustering. Subclustering is the process of clustering, subsetting to one cluster, then running the clustering pipeline again. In high-dimensional datasets, especially ones with lots of technical or biological noise, focusing on specific celltypes individually improves detection of subtype and state patterns. Highly variable gene selection and latent-space calculation are both affected by noise and outliers. Subclustering can also improve computational efficiency - finding small clusters can be expensive if working with the full dataset. 

However, it's important to keep in mind that iterative subclustering can lead to "overfitting" the data. This means we might identify noise as clusters, and we will have to contend more with the "curse of dimensionality" in downstream analysis. We should always validate our clusters according to expression of marker genes, use technical replicates, bootstrapping methods, or check their existence in external datasets. 

### Vocabulary

**Subclustering**

The process of dividing a previously identified cluster into smaller, more detailed clusters (subclusters). Subclustering is used to uncover finer, often subtle distinctions within a dataset that might not be visible in an initial analysis.

**Overfitting**

Overfitting is when an analyst describes random noise in the data rather than underlying general relationships. Overfit models perform well on their dataset, but very poorly on other data. 

**Curse of Dimensionality**

The term data analysts use to describe phenomena that appear when analyzing data in high-dimensional spaces. As dimensionality increases, the data can become sparse. Sparsity is problematic for statistical significance testing. Additionally, by increasing dimensions, we increase the number of false positives when using p-value thresholds.

**Parameter scan** 

AKA parameter sweep, this is the process of systematically varying parameters in an algorithm to analyze the effects of their changes on the outcome. Parameter scans are widely used in computationaly biology to identify optimal parameters or test the stability of our models. 

## Additional Reading



## Key Takeaways

- 

## Libraries

```{r message=FALSE, warning=FALSE}

### Make sure all the packages are installed
if (!requireNamespace("Seurat", quietly = TRUE))
    install.packages("Seurat")

if (!requireNamespace("tidyverse", quietly = TRUE))
    install.packages("tidyverse")

if (!requireNamespace("devtools", quietly = TRUE))
    install.packages("devtools")

if (!requireNamespace("colorBlindness", quietly = TRUE))
    install.packages("colorBlindness")

if (!requireNamespace("DT", quietly = TRUE))
    install.packages("DT")    

if (!requireNamespace("scales", quietly = TRUE))
    install.packages("scales") 

if (!requireNamespace("tictoc", quietly = TRUE))
    install.packages("tictoc") 

if (!requireNamespace("ggalluvial", quietly = TRUE))
    install.packages("ggalluvial") 

### Load all the necessary libraries
library(Seurat)
library(tidyverse)
library(devtools)
library(colorBlindness)
library(DT)
library(scales)
library(RColorBrewer)
library(scales)
library(tictoc)
library(ggalluvial)

set.seed(687)
```

## Load data

We're going to be working with a dataset from the paper - [Immunophenotyping of COVID-19 and influenza highlights the role of type I interferons in development of severe COVID-19](https://doi.org/10.1126/sciimmunol.abd1554) Download data from the [cellxgene](https://cellxgene.cziscience.com/collections/4f889ffc-d4bc-4748-905b-8eb9db47a2ed) portal.

```{r message=FALSE, warning=FALSE, output=FALSE}
se <- readRDS("../data/Covid_Flu_Seurat_Object.rds")
```

### Color palette

```{r}
donor_pal <- c(
    "#66C2A4", "#41AE76", "#238B45", "#006D2C",
    "#41B6C4", "#1D91C0", "#225EA8", "#253494", "#081D58",
    "#FFEDA0", "#FED976", "#FEB24C", "#FD8D3C",
    "#FC4E2A", "#E31A1C", "#BD0026", "#800026")

names(donor_pal) <- c(
    "Normal 1", "Normal 2", "Normal 3", "Normal 4",
    "Flu 1", "Flu 2", "Flu 3", "Flu 4", "Flu 5",
    "nCoV 1", "nCoV 2", "nCoV 3_4", "nCoV 5",
    "nCoV 6", "nCoV 7_8", "nCoV 9_10", "nCoV 11"  
)
```

### To get up to speed with the previous worksheets, process the data in the same way.

```{r}
se <- se %>%
    NormalizeData(verbose = FALSE) %>%
    FindVariableFeatures(
        method = "vst",
        nfeatures = 3000,
        verbose = FALSE) %>%
    ScaleData(verbose = FALSE, features = VariableFeatures(.)) %>%
    RunPCA() %>%
    FindNeighbors() %>%
    FindClusters(resolution = 0.05)
```

## Analysis

### Finding rare celltypes

For many of us, our first idea for finding rare celltypes is to modulate the parameters of what we have already to find the right celltypes. As we saw in the previous clustering notebook, we can find NK and T cell clusters this way, but it still seems like there's some heterogeneity in the clusters we've found. For illustration, I've run a parameter scan on the following variables:

- FindVariableFeatures nfeatures
- RunPCA npcs, 
- FindNeighbors k.param, 
- FindClusters resolution

```{r}
parameter_df <- expand.grid(
  nf = c(2000, 3000, 5000),
  pc = c(20, 30, 50),
  k = c(10, 30, 50),
  res = c(0.5, 0.8, 1.2)
)

paramscan <- readRDS('../data/covid_flu_srobj_clusters_paramscan.rds')
paramscan <- bind_cols(paramscan)

row.names(paramscan) <- colnames(se)

paramscan_long <- paramscan %>%
  rownames_to_column(var = "cell_id") %>%
  pivot_longer(
    cols = -cell_id,
    names_to = "parameter",
    values_to = "cluster"
  ) %>%
  mutate(
    nf = as.numeric(gsub(".*nf(\\d+)_.*", "\\1", parameter)),
    pc = as.numeric(gsub(".*pc(\\d+)_.*", "\\1", parameter)),
    k = as.numeric(gsub(".*k(\\d+)_.*", "\\1", parameter)),
    res = as.numeric(gsub(".*res(\\d+\\.\\d+).*", "\\1", parameter))
  ) %>% 
  group_by(parameter) %>% 
  mutate(
    n_clusts = max(as.numeric(cluster))
  ) %>%
  select(-parameter)

ggplot(paramscan_long %>% 
        select(-cell_id, -cluster) %>% 
        distinct, aes(x = factor(pc), 
        y = n_clusts, 
        fill = factor(k))) +
  geom_bar(stat='identity') +
  facet_grid(paste('k=',k) + paste('nf =',nf) ~ paste('resolution =',res)) +
  scale_y_continuous(labels = scales::label_number()) +
  scale_fill_manual(values=unname(donor_pal[c(1,6,11)])) +
  labs(title = "Number of clusters Across Parameters",
       x = "Clustering resolution + nPCs", y = "Number of clusters",
       fill = "k param") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```

Take a close look at the results. When we modulate nfeatures or nPCs, we don't directly see changes in the number of celltypes found. But, like we saw in the previous sessions, modulating the k.param and the clustering resolution have outsized effects on the number of clusters found.

But if we're looking for a rare celltype, we must include more information, right? It makes most sense to increase both the nfeatures, nPCs, AND clustering resolution to find that celltype - because we need to make sure we are including the genes that define the celltype, and making small enough clusters to be able to find it. 

So let's take a closer look at what changes in the PCA latent spaces. Maybe the information we need is there. 

### Comparing high and low parameterized PCA spaces

Here we will compare latent spaces between a low-parameter PCA and a high-parameter PCA. In the low, 2000 features and 20 PCs. In the high, 5000 features and 50 PCs. To keep a rational mind about the subject, let's time our processing runs to see how much more resources the high-param method needs.


```{r}


tic()
se <- se %>%
    FindVariableFeatures(
            method = "vst",
            nfeatures = 2000,
            verbose = FALSE) %>%
        ScaleData(verbose = FALSE, features = VariableFeatures(.)) %>% 
        RunPCA(
            npcs = 20,
            verbose = FALSE
        )
toc()

latent_l <- se@reductions$pca@cell.embeddings
loadings_l <- se@reductions$pca@feature.loadings
var_l <- se@reductions$pca@stdev
t_l <- se@reductions$pca@misc$total.variance

tic()
se <- se %>%
    FindVariableFeatures(
            method = "vst",
            nfeatures = 5000,
            verbose = FALSE) %>%
        ScaleData(verbose = FALSE, features = VariableFeatures(.)) %>% 
        RunPCA(
            npcs = 50,
            verbose = FALSE
        )
toc()

latent_h <- se@reductions$pca@cell.embeddings
loadings_h <- se@reductions$pca@feature.loadings
var_h <- se@reductions$pca@stdev
t_h <- se@reductions$pca@misc$total.variance

var_l_df <- data.frame(PC = 1:length(var_l), Variance = var_l^2 / t_l, Set = "2000 Features")
var_h_df <- data.frame(PC = 1:length(var_h), Variance = var_h^2 / t_h, Set = "5000 Features")

# Calculate cumulative variance explained
var_l_df$CumulativeVariance <- cumsum(var_l_df$Variance)
var_h_df$CumulativeVariance <- cumsum(var_h_df$Variance)

# Combine the datasets
combined_variance <- bind_rows(var_l_df, var_h_df)

# Plotting the variance explained and cumulative variance
ggplot(combined_variance, aes(x = PC)) +
  geom_line(aes(y = CumulativeVariance, color = Set), size = 1.2) +
  geom_point(aes(y = CumulativeVariance, color = Set), size = 2) +
  scale_color_manual(values = c("blue", "red")) +
  scale_linetype_manual(values = c("solid", "dashed")) +
  theme_minimal() +
  labs(title = "Cumulative Variance Explained",
       x = "Principal Component",
       y = "Proportion of Variance Explained") +
  guides(color = guide_legend(title = "Parameter Set"), 
         linetype = guide_legend(title = "Variance Type"))

```

Superficially, it looks like we are capturing more of the variance with our 2k features + 20PCs! But keep in mind if we only include 2000 features, we have much lower total variance. What we can see here is that the elbow comes sooner with the higher-parameterized set.

```{r}
# Ensure both matrices have the same genes in the same order
common_genes <- intersect(rownames(loadings_l), rownames(loadings_h))
loadings_l_aligned <- loadings_l[common_genes, , drop = FALSE]
loadings_h_aligned <- loadings_h[common_genes, , drop = FALSE]

# Determine high loadings
cutoff_l <- quantile(abs(as.matrix(loadings_l_aligned)), 0.95)
cutoff_h <- quantile(abs(as.matrix(loadings_h_aligned)), 0.95)

high_loadings_l <- apply(abs(loadings_l_aligned), 1, max) > cutoff_l
high_loadings_h <- apply(abs(loadings_h_aligned), 1, max) > cutoff_h

# Identify features that are high in _h but not high in _l
target_features <- names(which(high_loadings_h & !high_loadings_l))

# Prepare loadings of these target features for plotting
target_loadings <- loadings_h_aligned[target_features, , drop = FALSE]

# Convert to long format for ggplot using pivot_longer
loadings_long <- as.data.frame(target_loadings) %>%
  tibble::rownames_to_column("Feature") %>%
  pivot_longer(
    cols = -Feature,
    names_to = "PC",
    values_to = "Loading"
  )

compare_pca_loadings <- function(loadings1, loadings2, top_n = 20, cell_type_markers = NULL,
                                  loadings1_name = deparse(substitute(loadings1)), 
                                  loadings2_name = deparse(substitute(loadings2))) {
    all_genes <- union(rownames(loadings1), rownames(loadings2))

    # Initialize matrices to store aligned loadings with genes set to 0 by default
    loadings1_aligned <- matrix(0, nrow = length(all_genes), ncol = ncol(loadings1), 
                                dimnames = list(all_genes, colnames(loadings1)))
    loadings2_aligned <- matrix(0, nrow = length(all_genes), ncol = ncol(loadings2), 
                                dimnames = list(all_genes, colnames(loadings2)))

    # Fill in the existing values from each matrix
    loadings1_aligned[rownames(loadings1), ] <- loadings1
    loadings2_aligned[rownames(loadings2), ] <- loadings2

    # Check for zero values immediately after filling in values
    loadings1_zero <- apply(loadings1_aligned, 1, function(x) all(x == 0))
    loadings2_zero <- apply(loadings2_aligned, 1, function(x) all(x == 0))

    outlines <- character(length(all_genes))
    outlines[loadings1_zero] <- "orange"
    outlines[loadings2_zero] <- "blue"
    # outlines[loadings1_zero & loadings2_zero] <- "white" # Both are zero

    # Align the number of PCs if different
    min_pcs <- min(ncol(loadings1_aligned), ncol(loadings2_aligned))
    loadings1_aligned <- loadings1_aligned[, 1:min_pcs]
    loadings2_aligned <- loadings2_aligned[, 1:min_pcs]

    # Calculate signed differences in loadings
    loadings_difference <- loadings2_aligned - loadings1_aligned

    # Summarize the differences to find the genes with the greatest changes
    differences_summarized <- rowSums(abs(loadings_difference))
    directionality <- rowSums(loadings_difference)

    # Create a data frame for plotting
    genes_differences_df <- data.frame(
        Feature = names(differences_summarized),
        TotalDifference = directionality,
        AbsoluteDifference = differences_summarized,
        Direction = ifelse(directionality > 0, paste("Higher in", loadings2_name), paste("Higher in", loadings1_name)),
        Outline = outlines[match(names(differences_summarized), all_genes)]
    )


    fillpal <- c('steelblue','salmon')
    names(fillpal) = c(paste("Higher in", loadings2_name), paste("Higher in", loadings1_name))

    if (!is.null(cell_type_markers)) {
        marker_genes <- unlist(cell_type_markers, use.names = FALSE)
        genes_differences_df <- genes_differences_df %>%
            dplyr::filter(Feature %in% marker_genes) %>%
            dplyr::mutate(CellType = NA)  # Assign NA initially

        for (cell_type in names(cell_type_markers)) {
            genes_differences_df$CellType[genes_differences_df$Feature %in% cell_type_markers[[cell_type]]] <- cell_type
        }
        
        
        plot <- ggplot(genes_differences_df, aes(x = reorder(Feature, TotalDifference), y = TotalDifference, fill = Direction)) +
            geom_col() +
            coord_flip() +
            theme_minimal() +
            scale_fill_manual(values = fillpal) +
            labs(title = "Gene Loadings Differences by Cell Type",
                 subtitle = paste("Comparing", loadings1_name, "and", loadings2_name),
                 x = "Gene",
                 y = "Difference in Loadings") +
            guides(fill = guide_legend(title = "Where Loadings are Higher")) +
            facet_wrap(~CellType, scales = "free_y", ncol = 1, drop = TRUE)
    } else {
        genes_differences_df <- genes_differences_df %>%
            dplyr::arrange(desc(AbsoluteDifference)) %>%
            dplyr::slice(1:top_n)

        plot <- ggplot(genes_differences_df, aes(x = reorder(Feature, TotalDifference), y = TotalDifference, fill = Direction)) +
            geom_col(show.legend = FALSE) +
            scale_color_manual(name = "Outline Color", values = c("green" = "green", "blue" = "blue", "purple" = "purple"), labels = c(paste("Zero in", loadings1_name), paste("Zero in", loadings2_name), "Zero in Both")) +
            coord_flip() +
            theme_minimal() +
            scale_fill_manual(values = fillpal) +
            labs(title = paste("Top", top_n, "Genes with Greatest Differences in Loadings"),
                subtitle = paste("Comparing", loadings1_name, "and", loadings2_name),
                x = "Gene",
                y = "Difference in Loadings") +
            guides(fill = guide_legend(title = "Where Loadings are Higher"))
    }

    return(plot)
}

# Example usage:
compare_pca_loadings(loadings_l, loadings_h, top_n = 40)

```

Many of these genes are trained immunity genes. At this point we could keep some rare positive-control celltype in mind, like ILC3s or something else we might expect to find in this dataset

```{r}
# Define a list of cell type markers
cell_type_markers <- list(
  ILC3s = c("IL7RA", "PTGDR2", "NCR2", "CCR6", "RORC"),
  NKTfh = c("CD1D", "CXCR5", "PDCD1", "ICOS", "BCL6"),
  Plasmablasts = c("CD38" = "CD38", "CD27" = "CD27", "CD319" = "CD319", "IRF4" = "IRF4", "XBP1" = "XBP1"),
  MDSCs = c("CD33" = "CD33", "CD11b" = "ITGAM", "S100A8" = "S100A8", "S100A9" = "S100A9", "ARG1" = "ARG1")
)

compare_pca_loadings(loadings_l, loadings_h, cell_type_markers = cell_type_markers)
```

## Comparison to subclustering
```{r}
se <- se %>%
    FindVariableFeatures(
            method = "vst",
            nfeatures = 2000,
            verbose = FALSE) %>%
        ScaleData(verbose = FALSE, features = VariableFeatures(.)) %>% 
        RunPCA(
            npcs = 20,
            verbose = FALSE
        ) %>%
        FindNeighbors() %>%
        FindClusters(resolution = 0.05) %>%
        RunUMAP(dims = 1:20)

tse <- subset(se, idents = 0)

tse <- tse %>% 
    FindVariableFeatures(
            method = "vst",
            nfeatures = 2000,
            verbose = FALSE) %>%
        ScaleData(verbose = FALSE, features = VariableFeatures(.)) %>% 
        RunPCA(
            npcs = 20,
            verbose = FALSE
        ) %>%
    FindNeighbors() %>%
    FindClusters(resolution = 0.05) %>%
    RunUMAP(dims = 1:20)



DimPlot(tse, reduction='umap')
```

What you're probably wondering first is, hey would I have found these clusters if I had just increased the number of clusters I used? 
```{r}

tse_barcodes <- colnames(tse)
# subset paramscan to only subclustering cells
paramscan_subset <- paramscan[rownames(paramscan) %in% tse_barcodes, ]

# Select the relevant columns
paramscan_relevant <- paramscan_subset %>%
    select(nf5000_pc50_k10_res0.5, nf2000_pc20_k10_res0.5) %>%
    mutate(seurat_clusters = tse@meta.data[rownames(paramscan_subset),]$seurat_clusters,
    Celltype = tse@meta.data[rownames(paramscan_subset),]$Celltype)

# Reshape data for plotting
data_alluvial <- pivot_longer(paramscan_relevant,
                              cols = c("nf5000_pc50_k10_res0.5", "nf2000_pc20_k10_res0.5"),
                              names_to = "parameter",
                              values_to = "cluster")

data_alluvial <- data_alluvial %>% rename(subcluster = seurat_clusters,
                                          paramcluster = cluster) %>%
                                    count(parameter,subcluster,paramcluster,Celltype)

ggplot(data = data_alluvial,
       aes(axis1 = paramcluster, axis2 = subcluster, axis3 = Celltype, y = n)) +
    geom_alluvium(aes(fill = subcluster), width = 0.1) +
    geom_stratum(width = 0.1) +
    geom_text(stat = "stratum", aes(label = after_stat(stratum))) +
    theme_minimal() +
    facet_wrap(. ~ parameter) +
    labs(title = "Alluvial plot of parameter scans vs Seurat clusters",
         x = "Parameter combination",
         y = "Cluster assignment")

```

And it's true, you would have. But the path to getting there is different. 

If we check for example the overlap of highly variable features between the two methods, we'll find there are very few overlapping

```{r}
nh <- setdiff(VariableFeatures(se), VariableFeatures(tse)) %>% length

paste('Number of genes in 5k variable features not in the T cell subset:', nh)

paste('Number of genes in 2k variable features in the T cell subset:', 
setdiff(VariableFeatures(tse), VariableFeatures(se)) %>% length)

```

And if we compare their PCAs, we see that the greatest differences in the PCAs towards the T cell subset are genes very important to T cell subset status 
```{r}

loadings_t_subclustering <- tse@reductions$pca@feature.loadings
var_t <- tse@reductions$pca@stdev

loadings_nf2k_20pcs <- loadings_l

compare_pca_loadings(loadings_t_subclustering, loadings_nf2k_20pcs, top_n = 40)

loadings_nf5k_50pcs <- loadings_h

compare_pca_loadings(loadings_t_subclustering, loadings_nf5k_50pcs, top_n = 40)

```

## Session Info
```{r}
sessionInfo()
```