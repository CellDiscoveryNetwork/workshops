---
title: "3 - Normalization & Highly Variable Gene Selection"
author: "CDN team"
date: "`r format(Sys.time(), '%d %B, %Y')`"
format:
  html:
    toc: true
    toc_float: true
    toc-location: left
    toc-depth: 4
    html-math-method: katex
    embed-resources: true
    self-contained-math: true
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, out.width = "100%", fig.align='center', 
                      message = FALSE, warning = FALSE, cache = FALSE)
options(width = 1200)
```

## Introduction

In this notebook we are going to look at two big concepts: 1) Why we need to normalize the data and how we do that and 2) once the data is normalized, why we subset our data to the highly variable genes (HVG) and how to go about it. Normalization is a key step in the analysis. 

1) The analysis starts with a count matrix representing how many UMIs per gene were detected for each cell. These gene counts can be affected by technical variation such as the molecule capture rate, reverse transcription efficiency and sequencing depth. Therefore, when comparing gene expression between cells we need to adjust for library size.

2) Feature selection is necessary to reduce the dimensionality of the dataset while preserving as much biological information as possible. Highly variable genes are selected following the assumption that genes that vary throughout the data will represent different cell types. Genes that have a homogeneous expression are less interesting.


### Glossay

- **Unique molecular identifier (UMI)**: unique molecular barcode added to each transcript before PCR amplification to correct for biases during that process.

- **Library size**: total number of UMIs detected for each cell.

- **Library complexity**: number of genes that have expression > 0 in each cell. The more genes detected in a cell the more complex a cell is. Examples of low complexity cells are platelets and red blood cells which can be confounded with low quality cells if not assessed properly.


### Key Takeaways

-   Data normalization is key to correct for library size variability induced by technical artifacts.

-   Log(x+1) is typically carried out after library size correction, this has three important effects:

    1. Distances in between log values represent log fold changes
    
    2. Log transformation mitigates the mean-variance relationship
    
    3. Reduces the skewness of the data fulfilling the assumptions of downstream tools

-   Feature selection is the first step to reduce the dimensionality of our data.

-   We select highly variable genes (HVGs) following the assumption that genes that have variable expression are representative of cellular heterogeneity within our data.

-   Feature selection using HVGs is a KEY step in the analysis workflow. It determines the biological information universe we will be using for downstream analysis and is dependent on the data used. Every time we subset the data we have to recompute HVG to update our biological universe so it's representative of the new cellular context!

## Libraries

```{r message=FALSE, warning=FALSE}
### Make sure all the packages are installed
if (!requireNamespace("Seurat", quietly = TRUE))
    install.packages("Seurat")

if (!requireNamespace("tidyverse", quietly = TRUE))
    install.packages("tidyverse")

if (!requireNamespace("sparseMatrixStats", quietly = TRUE))
    install.packages("sparseMatrixStats")

if (!requireNamespace("colorBlindness", quietly = TRUE))
    install.packages("colorBlindness")

if (!requireNamespace("RColorBrewer", quietly = TRUE))
    install.packages("RColorBrewer")

### Load all the necessary libraries
library(Seurat)
library(tidyverse)
library(colorBlindness)
library(RColorBrewer)
```

## Load data

We're going to be working with a dataset from the paper - [Immunophenotyping of COVID-19 and influenza highlights the role of type I interferons in development of severe COVID-19](https://doi.org/10.1126/sciimmunol.abd1554) Download data from [cellxgene](https://cellxgene.cziscience.com/collections/4f889ffc-d4bc-4748-905b-8eb9db47a2ed) portal.

```{r message=FALSE, warning=FALSE, output=FALSE}
# Download the data in data/ directory
# download.file(
#     url = "https://datasets.cellxgene.cziscience.com/d8e35450-de43-451a-9979-276eac688bce.rds",
#     destfile = "../data/workshop-data.rds",
#     method = "wget",
#     extra = "-r -p --random-wait")
# We can also use the CLI with the wget command below
# wget https://datasets.cellxgene.cziscience.com/d8e35450-de43-451a-9979-276eac688bce.rds

se <- readRDS("../data/d8e35450-de43-451a-9979-276eac688bce.rds")
```

Generate a color palette for our cell types

```{r}
# https://www.datanovia.com/en/blog/easy-way-to-expand-color-palettes-in-r/
# nb.cols <- length(unique(se$Celltype))
# mycolors <- colorRampPalette(paletteMartin)(nb.cols)
pal <- paletteMartin
names(pal) <- sort(unique(se$Celltype))
```

## Analysis

### Convert ENSEMBL IDs to Gene Symbols

Right away we can see how ensembl ids are used in the rownames. Let's transform them into their matched symbols to make them human-readable:

```{r}
head(rownames(se))
```

Convert to gene symbols

```{r}
gene_df <- readr::read_csv(file = "../data/cov_flu_gene_names_table.csv")

symbol_id <- data.frame(index = rownames(se)) %>%
    left_join(gene_df, by = "index") %>%
    pull(feature_name)

# re-create seurat object
mtx <- se@assays$RNA$data
rownames(mtx) <- symbol_id
se <- CreateSeuratObject(counts = mtx, meta.data = se@meta.data)
```

Save object with gene symbols for use in future notebooks as a `.rds` file.
```{r}
saveRDS(object = se, file = "../data/Covid_Flu_Seurat_Object.rds")
```

### Data exploration

Let's start by taking a look at the sequencing depth of our cells. By library depth we referr to the number of transcripts (UMIs) detected in each cell
```{r}
ggplot(se@meta.data, aes(x = nCount_RNA)) +
    geom_density(color = "#6abcb6", fill = "#6abcb6", alpha = 0.7) +
    scale_x_continuous(
        transform = "log10",
        labels = scales::unit_format(unit = "K", scale = 1e-3)) +
    theme_classic()
```

Let's look at it by cell type as well
```{r}
ggplot(se@meta.data,
       aes(x = nCount_RNA, color = Celltype, fill = Celltype)) +
    geom_density(alpha = 0.7) +
    geom_vline(xintercept = 3000, linetype = "dashed", color = "red") +
    geom_vline(xintercept = 25000, linetype = "dashed", color = "red") +
    geom_text(aes(x = 3000, y = 0, label = "3K"), nudge_y = -0.05, color = "black") +
    geom_text(aes(x = 25000, y = 0, label = "25K"), nudge_y = -0.05, color = "black") +
    scale_x_continuous(
        transform = "log10",
        labels = scales::unit_format(unit = "K", scale = 1e-3)) +
    theme_classic() +
    scale_color_manual(values = pal) +
    scale_fill_manual(values = pal)
```

We can see how there is widespread variability in the library size across cells even from the same cell type. This is an important technical confounder we need to correct for. To correct for this, `Seurat` offers the function `NormalizeData` with normalization.method = *LogNormalize*. According to the documentation this function does the following - *Feature counts for each cell are divided by the total counts for that cell and multiplied by the scale.factor. This is then natural-log transformed using log1p*

Basically what [Seurat](https://github.com/satijalab/seurat/blob/1549dcb3075eaeac01c925c4b4bb73c73450fc50/R/preprocessing5.R#L265) is doing under the hood is:
```{r, eval = FALSE}
log1p(x = x / sum(x) * scale.factor)
```

Three things are happening:
1. `x / sum(x)` : divides individual counts for each cell by the total counts for that cell
2. `* scale.factor` : multiplies the results by a scaling factor (Default 1e4).
3. `log1p` : computes the log of the resulting value adding a pseudocount of 1 in case there are 0s.

We carry out these steps so that: 1) we account for technical variability in sequencing depth. Therefore we are now working with % of that gene vs all the rest. 2) so we're not working with small numbers (1e-4) which can rapidly decrease to 0 if we multiplied them for example. 3) stabilize the variance across different expression levels and thus reducing the influence of highly expressed genes and amplifying that of lowly expressed ones. As a bonus, using log-normalization is very interpretable since we know exactly what the values mean after applying the function.

### Variance stabilization

This post explains it very well - https://www.nxn.se/valent/2017/10/15/variance-stabilizing-scrna-seq-counts

If we take a look at what variance stabilization means we can see the following. When the data is uncorrected:
```{r fig.width=6, fig.height=6}
se@assays$RNA$counts[1:15, 1:15]
raw_sd <- sparseMatrixStats::rowSds(se@assays$RNA$counts)
raw_mn <- sparseMatrixStats::rowMeans2(se@assays$RNA$counts)

# Visualize
ggplot(mapping = aes(x = raw_mn, y = raw_sd)) +
    geom_point(alpha = 0.25) +
    theme_minimal() +
    labs(
        title = "Relation between raw gene expression Mean and Standard Deviation",
        x = "Gene Expression Mean",
        y = "Gene Expression SD"
    )
```

As we can observe, in general there is this relationship between the mean and the sd of the genes. If we normalize the data we can see how this relationship is greatly mitigated.

```{r fig.width=6, fig.height=6}
se <- NormalizeData(se, normalization.method = "LogNormalize", scale.factor = 1e4)

se@assays$RNA$data[1:15, 1:15]
norm_sd <- sparseMatrixStats::rowSds(se@assays$RNA$data)
norm_mn <- sparseMatrixStats::rowMeans2(se@assays$RNA$data)

# Visualize
ggplot(mapping = aes(x = norm_mn, y = norm_sd)) +
    geom_point(alpha = 0.25) +
    theme_minimal() +
    labs(
        title = "Relation between normalized gene expression Mean and Standard Deviation",
        x = "Gene Expression Mean",
        y = "Gene Expression SD"
    )
```

Let's take a look at which are the most highly expressed genes and why we might not be interested in them that much:
```{r fig.width=6, fig.height=6}
names(raw_mn) <- rownames(se@assays$RNA$counts)
data.frame(mean_expr = raw_mn) %>%
    rownames_to_column("gene") %>%
    arrange(desc(mean_expr)) %>%
    head(50) %>%
    ggplot(aes(x = mean_expr, y = forcats::fct_reorder(gene, mean_expr))) +
    geom_point() +
    theme_classic() +
    labs(x = "Mean gene expression", y = "")
```

Most of these genes are housekeeping, mitochondrial and ribosomal, which in most cases we are not interested about.

## Highly variable gene selection

Subsetting our count matrix to highly variable genes is a key step for downstream analysis. We need to reduce the dimensions of the count matrix so that we can carry out downstream tasks such as PCA. The intutition behind selectin HVG is that those genes with highest variability capture high level biological variability in our datasets and can help us identify the populations present in our dataset. To do this we can use the `vst` method: *First, fits a line to the relationship of log(variance) and log(mean) using local polynomial regression (loess). Then standardizes the feature values using the observed mean and expected variance (given by the fitted line). Feature variance is then calculated on the standardized values after clipping to a maximum (see clip.max parameter).*

We can see this in action below - Here we select 3000 HVG, in practice people select 2-5k genes in this step.
```{r fig.width=12, fig.height=9}
se <- FindVariableFeatures(se, nfeatures = 3000, selection.method = "vst")

# Identify the 50 most highly variable genes
top <- head(VariableFeatures(se), 100)

# plot variable features with and without labels
plot1 <- VariableFeaturePlot(se)
plot2 <- LabelPoints(plot = plot1, points = top, repel = TRUE)
plot2 +
    geom_smooth(method = "loess", error = FALSE) +
    scale_y_log10()
```

Selecting HVG is a key step in sc-RNAseq analysis since since it ultimately defines the biological space we will be moving in. So if we have a highly heterogeneous datasets with immune, epithelial, and stromal cells the HVG will capture high-level genes that will separate those major populations. Once we have those annotated, we will proceed to subsetting our data to a cell type of interest and **RECOMPUTING** HVG so that they are updated and represent the biological variability in this new dataset! 

Selecting HVG is probably the most important step in the analysis as it subset the genes to be used for downstream processing to compute PCA, knn-graph, clustering... Make sure this concept is clear before you move on!


## Session Info

```{r}
sessionInfo()
```
